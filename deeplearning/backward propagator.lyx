#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Backward propagator
\end_layout

\begin_layout Standard
In this note we find the formula for backward propagator algorithm in neutro
 network.
\end_layout

\begin_layout Standard
Suppose we have a network of 
\begin_inset Formula $L+1$
\end_inset

 layers with 
\begin_inset Formula $s_{0}$
\end_inset

, 
\begin_inset Formula $s_{1}$
\end_inset

, 
\begin_inset Formula $\cdots$
\end_inset

, 
\begin_inset Formula $s_{L-1}$
\end_inset

, 
\begin_inset Formula $s_{L}$
\end_inset

 non-constant nodes respectively.
 The 
\begin_inset Formula $0$
\end_inset

-th layer is the input layer and 
\begin_inset Formula $L$
\end_inset

-th layer is the output layer.
 For layer 
\begin_inset Formula $0$
\end_inset

 to 
\begin_inset Formula $L-1$
\end_inset

, each layer has a bias unit layer.
 
\end_layout

\begin_layout Standard
We use 
\begin_inset Formula $l$
\end_inset

 to label layers and 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $j$
\end_inset

 to label the nodes of layers.
 The parameter between layer 
\begin_inset Formula $l$
\end_inset

 and layer 
\begin_inset Formula $l+1$
\end_inset

 is 
\begin_inset Formula $\Theta_{i}^{(l)j}$
\end_inset

, which is an 
\begin_inset Formula $\left(s_{l+1}+1\right)\times\left(s_{l}+1\right)$
\end_inset

 dimensional matrix There are 
\begin_inset Formula $M$
\end_inset

 sets of training data.
 The 
\begin_inset Formula $m$
\end_inset

-th set of data at 
\begin_inset Formula $i$
\end_inset

-th node of 
\begin_inset Formula $l$
\end_inset

-th layer denoted as 
\begin_inset Formula $x_{i}^{(l)m}$
\end_inset

.
 In the computer programming language, the first index of matrix is always
 the lower index, i.e, 
\begin_inset Formula $x_{i}^{m}$
\end_inset

 is represented as 
\begin_inset Formula $x(i,m)$
\end_inset

 in computer programming language.
\end_layout

\begin_layout Standard
We also define 
\begin_inset Formula 
\[
z_{i}^{(l+1)m}=\Theta_{i}^{(l)j}x_{j}^{(l)m}=\Theta^{(l)}x^{(l)},\quad l=0,1,\dots,L-1
\]

\end_inset

 To take into account the bias unit node, we define 
\begin_inset Formula 
\[
\Theta_{0}^{(a)i}=\delta_{0}^{i},\quad a=0,1,\dots,L-1.
\]

\end_inset

At each node, there an activation 
\begin_inset Formula $g_{i}^{(l)}$
\end_inset

 at 
\begin_inset Formula $i$
\end_inset

-th node of 
\begin_inset Formula $l$
\end_inset

-th layer.
 So 
\begin_inset Formula $x_{i}^{(l)m}=g_{i}\left(z_{i}^{(l)m}\right)$
\end_inset

.
 To use matrix notation, let us suppose 
\begin_inset Formula 
\[
x_{i}^{(l)m}=g_{i}^{(l)}\left(z^{(l)m}\right).
\]

\end_inset

Now we have 
\begin_inset Formula $z^{(0)}=x^{(0)}$
\end_inset

 are input data at 
\begin_inset Formula $0$
\end_inset

-th layer.
 And 
\begin_inset Formula 
\[
x^{(1)m}=g^{(1)}\left(z^{(1)m}\right)=g^{(1)}\left(\Theta^{(0)}x^{(0)m}\right)=g^{(1)}\circ\Theta^{(0)}x^{(0)m}
\]

\end_inset


\begin_inset Formula 
\[
x^{(2)m}=g^{(2)}\left(z^{(2)m}\right)=g^{(2)}\left(\Theta^{(1)}x^{(1)m}\right)=g^{(2)}\circ\Theta^{(1)}x^{(1)m}=g^{(2)}\circ\Theta^{(1)}g^{(1)}\circ\Theta^{(0)}x^{(0)m}
\]

\end_inset

 So 
\begin_inset Formula 
\[
x^{(L)m}=g^{(L)}\circ\Theta^{(L-1)}g^{(L-1)}\circ\cdots\circ\Theta^{(1)}g^{(1)}\circ\Theta^{(0)}x^{(0)m}
\]

\end_inset

 So above are forward propagator algorithm.
\end_layout

\begin_layout Standard
Now consider the backward propagator.
 The loss function is 
\begin_inset Formula $J\left(x^{(L)m},y^{m}\right)$
\end_inset

.
 We want to find 
\begin_inset Formula 
\begin{align*}
\frac{\partial J_{\Theta}}{\partial\Theta_{i}^{(l)j}} & =\frac{\partial}{\partial\Theta_{i}^{(a)j}}J\left(x^{(L)1},y^{1},x^{(L)2},y^{2},\dots,x^{(L)M},y^{M}\right)\\
 & =\frac{\partial}{\partial\Theta_{i}^{(a)j}}J\left[g^{(L)}\circ\Theta^{(L-1)}g^{(L-1)}\circ\cdots\circ\Theta^{(a)}x^{(a)m},y^{m}\right]\\
 & =\frac{\partial}{\partial\Theta_{i}^{(a)j}}J\left[g^{(L)}\circ\Theta^{(L-1)}g^{(L-1)}\circ\cdots\circ\Theta^{(a+1)}g^{(a+1)}\circ z^{(a+1)m},y^{m}\right]\\
 & =\frac{\partial}{\partial z_{k}^{(l+1)m}}\left[J\circ g^{(L)}\circ\Theta^{(L-1)}g^{(L-1)}\circ\cdots\Theta^{(l+1)}g^{(l+1)}\right]\frac{\partial z_{k}^{(l+1)m}}{\partial\Theta_{i}^{(l)j}}\\
 & \equiv D_{m}^{(l+1)k}\frac{\partial\Theta_{k}^{(l)h}x_{h}^{(l)m}}{\partial\Theta_{i}^{(l)j}}\\
 & =D_{m}^{(l+1)k}\delta_{k}^{i}\delta_{j}^{h}x_{h}^{(l)m}\\
 & =D_{m}^{(l+1)i}x_{j}^{(l)m}\\
 & =\left(x^{(l)}D^{(l+1)}\right)_{ji}
\end{align*}

\end_inset

where 
\begin_inset Formula 
\[
D_{m}^{(l)i}\equiv\frac{\partial}{\partial z_{i}^{(l)m}}\left[J\circ g^{(L)}\circ\Theta^{(L-1)}g^{(L-1)}\circ\cdots\Theta^{(l)}g^{(l)}\left(z^{(l)}\right)\right]\Big|_{z^{(l)m}}
\]

\end_inset

 
\begin_inset Formula $x^{(l)}$
\end_inset

 is a 
\begin_inset Formula $\left(s_{l}+1\right)\times M$
\end_inset

 dimensional matrix and 
\begin_inset Formula $D^{(l+1)}$
\end_inset

 is a 
\begin_inset Formula $M\times s_{l+1}$
\end_inset

 dimensional matrix.
 
\end_layout

\begin_layout Standard
Now 
\begin_inset Formula 
\begin{align*}
{D_{m}^{(l-1)}}^{i} & =\frac{\partial}{\partial{z_{i}^{(l-1)}}^{m}}\left[J\circ g^{(L)}\circ\Theta^{(L-1)}g^{(L-1)}\circ\cdots\Theta^{(l-1)}g^{(l-1)}\right]\\
 & =\frac{\partial}{\partial{z_{i}^{(l-1)}}^{m}}\left[J\circ g^{(L)}\circ\Theta^{(L-1)}g^{(L-1)}\circ\cdots\circ\Theta^{(l-1)}g^{(l-1)}\left(z^{(l-1)\alpha}\right)\right]\\
 & =\frac{\partial}{\partial{z_{j}^{(l)}}^{m}}\left[J\circ g^{(L)}\circ\Theta^{(L-1)}g^{(L-1)}\circ\cdots\circ\Theta^{(l)}g^{(l)}\circ z^{(l)}\right]\frac{\partial z_{j}^{(l)m}}{\partial z_{i}^{(l-1)m}}\\
 & =D_{m}^{(l)j}\frac{\partial\left(\Theta^{(l-1)}g^{(l-1)}\left(z^{(l-1)}\right)\right)_{j}}{\partial z_{i}^{(l-1)m}}\\
 & =D_{m}^{(l)j}\Theta_{j}^{(l-1)k}\frac{\partial\left(g_{k}^{(l-1)}\left(z^{(l-1)m}\right)\right)}{\partial z_{i}^{(l-1)m}}\\
 & =D_{m}^{(l)j}\Theta_{j}^{(l-1)k}\left(\partial^{i}g_{k}^{(l-1)}\right)\Big|_{z^{(l-1)m}}
\end{align*}

\end_inset

Now assume for hidden layers 
\begin_inset Formula $g_{k}^{\left(l\right)}$
\end_inset

 only depends on 
\begin_inset Formula $z_{k}^{(l)m}$
\end_inset

, then 
\begin_inset Formula 
\[
\left(\partial^{i}g_{k}^{(l-1)}\right)\Big|_{z^{(a-1)m}}=\delta_{k}^{i}\left(g_{k}^{(l-1)\prime}\right)\Big|_{z_{i}^{(l-1)m}}
\]

\end_inset

 So 
\begin_inset Formula 
\begin{align*}
{D_{m}^{(l-1)}}^{i} & =D_{m}^{(l)j}\Theta_{j}^{(l-1)k}\delta_{k}^{i}\left(g_{k}^{(l-1)\prime}\right)\Big|_{z_{i}^{(l-1)m}}\\
 & =D_{m}^{(l)j}\Theta_{j}^{(l-1)i}\left(g^{(l-1)}\right)^{\prime}\Big|_{z_{i}^{(l-1)m}}\\
 & =\left(D^{(l)}\Theta^{(l-1)}\right).*\left(g^{(l-1)}\right)^{\prime}
\end{align*}

\end_inset

where 
\begin_inset Formula 
\[
\left[\left(g^{(l)}\right)^{\prime}\right]_{m}^{i}\equiv\left(g^{(l)}\right)^{\prime}\Big|_{z_{i}^{(l)m}}
\]

\end_inset

 Note that if we a node has one more output then input, i.e, it has bias
 unit node, then the index 
\begin_inset Formula $k$
\end_inset

 of 
\begin_inset Formula $g_{k}^{(l)}$
\end_inset

 runs over one additional value than the index 
\begin_inset Formula $i$
\end_inset

 in 
\begin_inset Formula $z_{i}^{(l)}$
\end_inset

.
 The kronecker 
\begin_inset Formula $\delta_{k}^{i}$
\end_inset

 effectively reduces range of 
\begin_inset Formula $k$
\end_inset

 to be the same as 
\begin_inset Formula $i$
\end_inset

.
 It implies that the range of upper index in 
\begin_inset Formula $\Theta_{j}^{(l-1)i}$
\end_inset

 should also be reduced by one.
\end_layout

\begin_layout Standard
The dimension of 
\begin_inset Formula $D^{(l)}$
\end_inset

 is 
\begin_inset Formula $M\times s_{l}$
\end_inset

.
 For the output layer, we need to compute 
\begin_inset Formula $D^{(L)}$
\end_inset

, 
\begin_inset Formula 
\begin{align*}
D_{m}^{(L)i} & \equiv\frac{\partial}{\partial z_{i}^{(L)m}}\left[J\circ g^{(L)}\left(z^{(L)}\right)\right]\\
 & =\left(\partial_{x_{j}^{m}}J\right)\frac{\partial g_{j}^{(L)}\left(z^{(L)m}\right)}{\partial z_{i}^{(L)m}}\Big|_{z^{(L)m}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Consider the classification case.
 The loss function
\begin_inset Formula 
\[
J=-\frac{1}{M}\sum_{m}\sum_{i}\left[\left(1-y_{i}^{m}\right)\log\left(1-x_{i}^{m}\right)+y_{i}^{m}\log x_{i}^{m}\right]
\]

\end_inset


\begin_inset Formula 
\begin{align*}
\partial_{x_{i}^{m}}J & =-\frac{1}{M}\left(-\frac{\left(1-y_{i}^{m}\right)}{1-x_{i}^{m}}+\frac{y_{i}^{m}}{x_{i}^{m}}\right)\\
 & =-\frac{1}{M}\left(\frac{-\left(1-y_{i}^{m}\right)x_{i}^{m}+y_{i}^{m}\left(1-x_{i}^{m}\right)}{x_{i}^{m}\left(1-x_{i}^{m}\right)}\right)\\
 & =-\frac{1}{M}\frac{y_{i}^{m}-x_{i}^{m}}{x_{i}^{m}\left(1-x_{i}^{m}\right)}.
\end{align*}

\end_inset

 For activation, if there is only one output node, we have
\begin_inset Formula 
\[
\frac{\partial g_{j}^{(L)}\left(z^{(L)m}\right)}{\partial z_{i}^{(L)m}}=\left(\frac{1}{1+e^{-z}}\right)^{\prime}=\frac{e^{-z}}{\left(1+e^{-z}\right)^{2}}=x^{2}\left(1/x-1\right)=x-x^{2}
\]

\end_inset

 so for classification of one output node.
\begin_inset Formula 
\[
D_{m}^{(L)j}=\frac{1}{M}\left(x_{i}^{m}-y_{i}^{m}\right)
\]

\end_inset


\end_layout

\begin_layout Section*
Softmax
\end_layout

\begin_layout Standard
For multi-class classification, the activation is the softmax equation
\begin_inset Formula 
\[
g_{j}\left(z_{m}\right)=p\left(y=j|z^{m}\right)=\frac{e^{z_{j}^{m}}}{\sum_{k}e^{z_{k}^{m}}},\quad z_{i}^{m}=\Theta_{i}^{(L-1)j}x_{j}^{(L-1)m}
\]

\end_inset

 So 
\begin_inset Formula 
\begin{align*}
\frac{\partial g_{j}^{(L)}\left(z^{(L)m}\right)}{\partial z_{i}^{(L)m}} & =\frac{\partial}{\partial z_{i}^{m}}\left(\frac{e^{z_{j}^{m}}}{\sum_{k}e^{z_{k}^{m}}}\right)\\
 & =\delta_{j}^{i}\frac{e^{z_{j}^{m}}}{\sum_{k}e^{z_{k}^{m}}}-\frac{e^{z_{j}^{m}}e^{z_{i}^{m}}}{\left(\sum_{k}e^{z_{k}^{m}}\right)^{2}}\\
 & =\delta_{j}^{i}g_{j}\left(z^{m}\right)-g_{i}g_{j}\\
 & =\delta_{j}^{i}x_{j}^{m}-x_{i}^{m}x_{j}^{m}
\end{align*}

\end_inset

 Let the nubmer of nodes in the output layer is 
\begin_inset Formula $K$
\end_inset

, then 
\begin_inset Formula 
\begin{align*}
D_{m}^{(L)i} & =\left(\partial_{x_{j}^{m}}J\right)\frac{\partial g_{j}^{(L)}\left(z^{(L)m}\right)}{\partial z_{i}^{(L)m}}\Big|_{z^{(L)m}}\\
 & =\left(\partial_{x_{j}^{m}}J\right)\left(\delta_{j}^{i}x_{j}^{m}-x_{i}^{m}x_{j}^{m}\right)\\
 & =\left(\partial_{x_{i}^{m}}J\right)x_{i}^{m}-\sum_{j=1}^{K}\left(\partial_{x_{j}^{m}}J\right)x_{i}^{m}x_{j}^{m}\\
 & =-\frac{1}{M}\frac{y_{i}^{m}-x_{i}^{m}}{x_{i}^{m}\left(1-x_{i}^{m}\right)}x_{i}^{m}+\frac{1}{M}\sum_{j}\frac{y_{j}^{m}-x_{j}^{m}}{x_{j}^{m}\left(1-x_{j}^{m}\right)}x_{i}^{m}x_{j}^{m}\\
 & =-\frac{1}{M}\frac{y_{i}^{m}-x_{i}^{m}}{\left(1-x_{i}^{m}\right)}+\frac{1}{M}\sum_{j=1}^{K}\frac{y_{j}^{m}-x_{j}^{m}}{\left(1-x_{j}^{m}\right)}x_{i}^{m}\\
 & =\frac{1}{M}\left(x_{i}^{m}-y_{i}^{m}\right)+\frac{1}{M}\sum_{j\neq i}\frac{y_{j}^{m}-x_{j}^{m}}{\left(1-x_{j}^{m}\right)}x_{i}^{m}
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
\frac{1}{M}\sum_{j\neq i}\frac{y_{j}^{m}-x_{j}^{m}}{\left(1-x_{j}^{m}\right)}x_{i}^{m} & =\frac{1}{M}\sum_{j\neq i}\frac{y_{j}^{m}-x_{j}^{m}}{\left(1-x_{j}^{m}\right)}\left(1-x_{j}-\sum_{k\neq i,j}x_{k}^{m}\right)\\
 & =\frac{1}{M}\sum_{j\neq i}\left(y_{j}^{m}-x_{j}^{m}\right)\left(1-\frac{\sum_{k\neq i,j}x_{k}^{m}}{\sum_{k\neq j}x_{k}^{m}}\right)\\
 & =\frac{1}{M}\left(x_{i}^{m}-y_{i}^{m}\right)-\frac{1}{M}\sum_{j\neq i}\left(y_{j}^{m}-x_{j}^{m}\right)\frac{\sum_{k\neq i,j}x_{k}^{m}}{\sum_{k\neq j}x_{k}^{m}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
For 
\begin_inset Formula $K=3$
\end_inset

, 
\begin_inset Formula 
\begin{align*}
\sum_{j\neq i}\frac{y_{j}^{m}-x_{j}^{m}}{\left(1-x_{j}^{m}\right)}x_{i}^{m} & =\frac{y_{2}-x_{2}}{\left(1-x_{2}\right)}x_{1}+\frac{y_{3}-x_{3}}{\left(1-x_{3}\right)}x_{1}\\
 & =\frac{\left(y_{2}-x_{2}\right)\left(1-x_{3}\right)+\left(y_{3}-x_{3}\right)\left(1-x_{2}\right)}{\left(1-x_{2}\right)\left(1-x_{3}\right)}x_{1}\\
 & =\frac{y_{2}+y_{3}-2\left(x_{2}+x_{3}\right)-y_{2}x_{3}-y_{3}x_{2}+2x_{2}x_{3}}{\left(1-x_{2}\right)\left(1-x_{3}\right)}x_{1}\\
 & =
\end{align*}

\end_inset


\end_layout

\end_body
\end_document
